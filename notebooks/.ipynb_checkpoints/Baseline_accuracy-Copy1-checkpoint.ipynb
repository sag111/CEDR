{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6150adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6544bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029d602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/aleksandr/.cache/huggingface/modules/datasets_modules/datasets/cedr/3206e7f85b26ab12b6269276b8afcc3bcb11971fca7d7e9154ab2c21dc90dfc0 (last modified on Fri Aug 13 12:30:56 2021) since it couldn't be found locally at /home/aleksandr/Документы/active/CEDR_0_1_1/notebooks/sagteam/cedr/cedr.py, or remotely (ImportError).\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f649a87f6a8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aleksandr/anaconda3/envs/git_cedr_v001/lib/python3.7/site-packages/tqdm/std.py\", line 1152, in __del__\n",
      "    self.close()\n",
      "  File \"/home/aleksandr/anaconda3/envs/git_cedr_v001/lib/python3.7/site-packages/tqdm/notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Reusing dataset cedr (/home/aleksandr/.cache/huggingface/datasets/cedr/enriched/0.1.1/3206e7f85b26ab12b6269276b8afcc3bcb11971fca7d7e9154ab2c21dc90dfc0)\n",
      "Using the latest cached version of the module from /home/aleksandr/.cache/huggingface/modules/datasets_modules/datasets/cedr/3206e7f85b26ab12b6269276b8afcc3bcb11971fca7d7e9154ab2c21dc90dfc0 (last modified on Fri Aug 13 12:30:56 2021) since it couldn't be found locally at /home/aleksandr/Документы/active/CEDR_0_1_1/notebooks/sagteam/cedr/cedr.py, or remotely (ImportError).\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f649a87f6a8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aleksandr/anaconda3/envs/git_cedr_v001/lib/python3.7/site-packages/tqdm/std.py\", line 1152, in __del__\n",
      "    self.close()\n",
      "  File \"/home/aleksandr/anaconda3/envs/git_cedr_v001/lib/python3.7/site-packages/tqdm/notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Reusing dataset cedr (/home/aleksandr/.cache/huggingface/datasets/cedr/enriched/0.1.1/3206e7f85b26ab12b6269276b8afcc3bcb11971fca7d7e9154ab2c21dc90dfc0)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "train_df = load_dataset('sagteam/cedr', name='enriched', split='train')\n",
    "test_df = load_dataset('sagteam/cedr', name='enriched', split='test')\n",
    "\n",
    "with open('../data/emo_lexicon.json') as f:\n",
    "    emo_lexicon_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b351928b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 1882, 9410)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df), len(train_df)+len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d83b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2162, 185, 89, 115, 112, 188)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0, n1, n2, n3, n4, n5 = 0,0,0,0,0,0\n",
    "for doc in list(train_df)+list(test_df):\n",
    "    if 'lenta' in doc['source']:\n",
    "        if doc['labels'] == []:\n",
    "            n0+=1\n",
    "        if 'joy' in doc['labels']:\n",
    "            n1+=1\n",
    "        if 'sadness' in doc['labels']:\n",
    "            n2+=1\n",
    "        if 'fear' in doc['labels']:\n",
    "            n3+=1\n",
    "        if 'anger' in doc['labels']:\n",
    "            n4+=1\n",
    "        if 'surprise' in doc['labels']:\n",
    "            n5+=1\n",
    "n0, n1, n2, n3, n4, n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb9bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1498, 437, 304, 265, 232, 397)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0, n1, n2, n3, n4, n5 = 0,0,0,0,0,0\n",
    "for doc in list(train_df)+list(test_df):\n",
    "    if 'lj' in doc['source']:\n",
    "        if doc['labels'] == []:\n",
    "            n0+=1\n",
    "        if 'joy' in doc['labels']:\n",
    "            n1+=1\n",
    "        if 'sadness' in doc['labels']:\n",
    "            n2+=1\n",
    "        if 'fear' in doc['labels']:\n",
    "            n3+=1\n",
    "        if 'anger' in doc['labels']:\n",
    "            n4+=1\n",
    "        if 'surprise' in doc['labels']:\n",
    "            n5+=1\n",
    "n0, n1, n2, n3, n4, n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fab8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 1300, 1403, 350, 192, 192)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0, n1, n2, n3, n4, n5 = 0,0,0,0,0,0\n",
    "for doc in list(train_df)+list(test_df):\n",
    "    if 'twitter' in doc['source']:\n",
    "        if doc['labels'] == []:\n",
    "            n0+=1\n",
    "        if 'joy' in doc['labels']:\n",
    "            n1+=1\n",
    "        if 'sadness' in doc['labels']:\n",
    "            n2+=1\n",
    "        if 'fear' in doc['labels']:\n",
    "            n3+=1\n",
    "        if 'anger' in doc['labels']:\n",
    "            n4+=1\n",
    "        if 'surprise' in doc['labels']:\n",
    "            n5+=1\n",
    "n0, n1, n2, n3, n4, n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafa589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e038ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e0feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random approach:\n",
      "\n",
      "Emotion \"surprise\":\n",
      "mic.: 0.51;\t mac.:0.41\n",
      "\n",
      "Emotion \"fear\":\n",
      "mic.: 0.5;\t mac.:0.39\n",
      "\n",
      "Emotion \"joy\":\n",
      "mic.: 0.51;\t mac.:0.46\n",
      "\n",
      "Emotion \"sadness\":\n",
      "mic.: 0.52;\t mac.:0.47\n",
      "\n",
      "Emotion \"anger\":\n",
      "mic.: 0.47;\t mac.:0.37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random approach\n",
    "\n",
    "print('Random approach:\\n')\n",
    "\n",
    "for emo_label in emo_lexicon_dict.keys():\n",
    "    true_y = []\n",
    "    \n",
    "    for sample in test_df:\n",
    "        if emo_label in sample['labels']:\n",
    "            true_y.append(1)\n",
    "        else:\n",
    "            true_y.append(0)\n",
    "            \n",
    "    # The emotion label is chosen randomly for each sentence\n",
    "    pred_y = np.random.randint(0,2,len(test_df))\n",
    "    \n",
    "    # The accuracy of the obtained models is measured with the F1 metric\n",
    "    p_micro, r_micro, f_micro, _ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"micro\")\n",
    "    p_macro, r_macro, f_macro, _ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"macro\")\n",
    "    \n",
    "    print(f'Emotion \"{emo_label}\":')\n",
    "    print(f'mic.: {round(f_micro, 2)};\\t mac.:{round(f_macro, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ac44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon approach:\n",
      "\n",
      "Emotion \"surprise\":\n",
      "mic.: 0.88;\t mac.:0.76\n",
      "\n",
      "Emotion \"fear\":\n",
      "mic.: 0.83;\t mac.:0.68\n",
      "\n",
      "Emotion \"joy\":\n",
      "mic.: 0.83;\t mac.:0.73\n",
      "\n",
      "Emotion \"sadness\":\n",
      "mic.: 0.71;\t mac.:0.62\n",
      "\n",
      "Emotion \"anger\":\n",
      "mic.: 0.76;\t mac.:0.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lexicon approach\n",
    "\n",
    "print('Lexicon approach:\\n')\n",
    "\n",
    "for emo_label in emo_lexicon_dict.keys():\n",
    "    true_y, pred_y = [], []\n",
    "\n",
    "    for sample in test_df:\n",
    "        if emo_label in sample['labels']:\n",
    "            true_y.append(1)\n",
    "        else:\n",
    "            true_y.append(0)\n",
    "        \n",
    "        # The emotion label is determined by thepresence of words from \n",
    "        # the emotive vocabulary for the corresponding emotion\n",
    "        sample_lemms = [word['lemma'].lower() for sentence in sample['sentences'] for word in sentence]        \n",
    "        if any(word in emo_lexicon_dict[emo_label] for word in sample_lemms):\n",
    "            pred_y.append(1)\n",
    "        else:\n",
    "            pred_y.append(0)\n",
    "    \n",
    "    # The accuracy of the obtained models is measured with the F1 metric\n",
    "    p_micro, r_micro, f_micro, _ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"micro\")\n",
    "    p_macro, r_macro, f_macro, _ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"macro\")\n",
    "    \n",
    "    print(f'Emotion \"{emo_label}\":')\n",
    "    print(f'mic.: {round(f_micro, 2)};\\t mac.:{round(f_macro, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc1c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(TF-IDF) model:\n",
      "\n",
      "Emotion \"surprise\":\n",
      "mic.: 0.93;\t mac.:0.67\n",
      "\n",
      "Emotion \"fear\":\n",
      "mic.: 0.94;\t mac.:0.66\n",
      "\n",
      "Emotion \"joy\":\n",
      "mic.: 0.86;\t mac.:0.67\n",
      "\n",
      "Emotion \"sadness\":\n",
      "mic.: 0.86;\t mac.:0.71\n",
      "\n",
      "Emotion \"anger\":\n",
      "mic.: 0.93;\t mac.:0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM + (TF-IDF)\n",
    "\n",
    "print('SVM(TF-IDF) model:\\n')\n",
    "\n",
    "# TF-IDF features\n",
    "vect = TfidfVectorizer(analyzer='char', ngram_range=(4,8))\n",
    "texts_for_fit = [sample['text'].lower() for sample in train_df]\n",
    "texts_for_fit += [sample['text'].lower() for sample in test_df]\n",
    "vect.fit(texts_for_fit)\n",
    "\n",
    "# SVM model with linear kernel\n",
    "model = LinearSVC(random_state=42)\n",
    "\n",
    "for emo_label in emo_lexicon_dict.keys():\n",
    "    train_x, test_x = [], []\n",
    "    train_y, true_y = [], []\n",
    "    pred_y = []\n",
    "    \n",
    "    for sample in train_df:\n",
    "        train_x.append(sample['text'].lower())\n",
    "        \n",
    "        if emo_label in sample['labels']:\n",
    "            train_y.append(1)\n",
    "        else:\n",
    "            train_y.append(0)\n",
    "\n",
    "    train_x = vect.transform(train_x)\n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    for sample in test_df:\n",
    "        test_x.append(sample['text'].lower())\n",
    "        \n",
    "        if emo_label in sample['labels']:\n",
    "            true_y.append(1)\n",
    "        else:\n",
    "            true_y.append(0)\n",
    "    \n",
    "    test_x = vect.transform(test_x)\n",
    "    pred_y = model.predict(test_x)\n",
    "    \n",
    "    # The accuracy of the obtained models is measured with the F1 metric\n",
    "    p_micro,r_micro,f_micro,_ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"micro\")\n",
    "    p_macro,r_macro,f_macro,_ = metrics.precision_recall_fscore_support(true_y, pred_y, average=\"macro\")\n",
    "    \n",
    "    print(f'Emotion \"{emo_label}\":')\n",
    "    print(f'mic.: {round(f_micro, 2)};\\t mac.:{round(f_macro, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "648a0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our approach:\n",
      "\n",
      "Emotion \"surprise\":\n",
      "mic.: 0.93;\t mac.:0.76\n",
      "\n",
      "Emotion \"fear\":\n",
      "mic.: 0.93;\t mac.:0.73\n",
      "\n",
      "Emotion \"joy\":\n",
      "mic.: 0.92;\t mac.:0.87\n",
      "\n",
      "Emotion \"sadness\":\n",
      "mic.: 0.92;\t mac.:0.86\n",
      "\n",
      "Emotion \"anger\":\n",
      "mic.: 0.9;\t mac.:0.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our approach (ensemble + ELMO embedding vectors)\n",
    "\n",
    "print('Our approach:\\n')\n",
    "\n",
    "# After the adjustment and comparison of the classifiers, \n",
    "# the final solution is an ensemble of the following five binaryclassifiers:\n",
    "\n",
    "# 1. A model based on gradient descent and logistic regression methods, \n",
    "# with preliminarynormalization of the data: the ratio of the difference in value and \n",
    "# its mean to standard deviation\n",
    "surprise_model = load_model('../models/surprise_model.pkl')\n",
    "\n",
    "# 2. A model based on stochastic gradient descent with PCA preprocessing of input features\n",
    "fear_model = load_model('../models/fear_model.pkl')\n",
    "\n",
    "# 3. A model based on logistic regression\n",
    "sad_model = load_model('../models/sad_model.pkl')\n",
    "\n",
    "# 4. A model based on a support vector machine with a linear kernel\n",
    "joy_model = load_model('../models/joy_model.pkl')\n",
    "\n",
    "# 5. A model based on logistic regression\n",
    "anger_model = load_model('../models/anger_model.pkl')\n",
    "\n",
    "# load ELMO embedding vectors\n",
    "df = load_model('../data/elmo_vec.pkl')\n",
    "\n",
    "for emo_label in emo_lexicon_dict.keys():\n",
    "    if emo_label == 'surprise':\n",
    "        model = surprise_model\n",
    "    elif emo_label == 'fear':\n",
    "        model = fear_model\n",
    "    elif emo_label == 'sadness':\n",
    "        model = sad_model\n",
    "    elif emo_label == 'joy':\n",
    "        model = joy_model\n",
    "    elif emo_label == 'anger':\n",
    "        model = anger_model\n",
    "        \n",
    "    train_x, train_y = [], []\n",
    "    test_x, test_y = [], []\n",
    "    \n",
    "    for sample in df['train']:\n",
    "        train_x.append(sample['vec'])\n",
    "        if emo_label in sample['labels']:\n",
    "            train_y.append(1)\n",
    "        else:\n",
    "            train_y.append(0)\n",
    "\n",
    "    for sample in df['test']:\n",
    "        test_x.append(sample['vec'])\n",
    "        if emo_label in sample['labels']:\n",
    "            test_y.append(1)\n",
    "        else:\n",
    "            test_y.append(0)\n",
    "    \n",
    "    pred_y = model.predict(np.array(test_x))\n",
    "\n",
    "    p_micro,r_micro,f_micro,_ = metrics.precision_recall_fscore_support(test_y, pred_y, average=\"micro\")\n",
    "    p_macro,r_macro,f_macro,_ = metrics.precision_recall_fscore_support(test_y, pred_y, average=\"macro\")\n",
    "    \n",
    "    print(f'Emotion \"{emo_label}\":')\n",
    "    print(f'mic.: {round(f_micro, 2)};\\t mac.:{round(f_macro, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11587fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Successful complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.7.0, cedr_v0.0.1)",
   "language": "python",
   "name": "git_cedr_v001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
